% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/UnsupRFhclust.R
\name{UnsupRFhclust}
\alias{UnsupRFhclust}
\alias{UnsupRFhclust.caret}
\alias{cluster_internal_validation}
\title{Unsupervised random forest with hclust: cluster data and evaluate predictive strength of clusters.}
\usage{
UnsupRFhclust(formula, dat, rfdist, hclust.method = "ward.D2", K = 10,
  parallel = TRUE, cv = 5, mc.cores = 2, seed = 12345,
  verbos = TRUE, ...)

UnsupRFhclust.caret(formula, dat, rfdist, hclust.method = "ward.D2",
  K = 10, parallel = TRUE, cv = 5, mc.cores = 2, seed = 12345,
  caret.method = "glm", fitControl = trainControl(method = "none",
  classProbs = TRUE), verbos = TRUE, ...)

cluster_internal_validation(K = 10, Hclust.model, RFdist,
  parallel = TRUE, mc.cores = 2, seed = 1234, ...)
}
\arguments{
\item{formula}{an R formuala. Note, only binary outcomes are currently supported.}

\item{dat}{data matrix}

\item{rfdist}{dissimilarity matrix, such as the RF distance matrix computed using \code{\link{RFdist}} 
based on the \code{dat} data matrix.}

\item{hclust.method}{the agglomeration method to be use see \code{link{hlust}}.}

\item{K}{maximum number of clusters to generate for hierarchical clustering model}

\item{parallel}{(logical) run in parallel ?}

\item{cv}{number of cross-validation to perform using data, must be at least 2}

\item{mc.cores}{number of cores}

\item{seed}{random seed}

\item{\dots}{further arguments passed to or from other methods.}

\item{caret.method}{classifcation  method to use in caret: "glm" and "rf" currently tested.}

\item{fitControl}{Control the computational nuances of the caret \code{\link{train}} function. 
See the \code{\link{caret}} package}

\item{Hclust.model}{hierarchical clustering model}

\item{RFdist}{RF distance matrix computed from \code{\link{RFdist}}.}
}
\value{
a data frame with columns: 
\enumerate{
\item UnsupRFhclust and UnsupRFhclust.caret
\enumerate{
  \item Hclust: hierarchical clustering model 
  \item perf: a data frame with columns: AUC, cluster (cluster numbers), CV (cross-validation number), and type 
(for one of the three types of models mention in the description)  
 }
\item cluster_internal_validation
\enumerate{
  \item clusters: cluster memberships for the optimal number of clusters (out put of cluster_internal_validation)
  \item kopt: best number of clusters obtained by majprity rule on the table results below. 
  \item table: matrix of internal validation metrics  (out put of cluster_internal_validation) with columns 
        \itemize{
        \item sep: cluster seperation. Higher values indicates better clustering 
        \item toother: sum of the average distances of a point in a cluster to points in other clusters. Higher values 
              indicates better clustering  
        \item within : sum of average distance within clusters: Smaller values indicates better clustering  
        \item between: average distance between clusters. Higher values indicates better clustering 
        \item ss: within clusters sum of squares. Smaller values indicates better clustering 
        \item silwidth: average silhouette width. Higher values indicates better clustering. See \code{\link{silhouette}}. 
        \item dunn: Dunn index. Higher values indicates better clustering
        \item dunn2: Another version of Dunn index
        \item wb.ratio: (negative) ratio of  average distance between clusters to average distance within clusters.  
               Smaller values indicates better clustering (or positive and large value is better)  
       \item ch: Calinski and Harabasz index. Higher values is better 
       \item entropy: negative entropy. Smaller values are better 
       \item w.gap:  sum of vector of widest within-cluster gaps. Small is better 
        }
  }
}
}
\description{
This function takes a dissimilarity matrix, such as the Random Forest dissimilarity matrix 
from \code{\link{RFdist}} and contructs a hirearchical clustering object using the \code{\link{hlust}} package. 
  It then evaluates the predictive ability of different clusterings k = 2:K by 
 predicting a binary response variable based on cluster memberships. The results can be used to 
validate and select the best number of clusters.  See Ngufor et al. 

 It takes a standard  \code{formula}, a data matrix \code{dat} containing the binary response, and a 
disimilarity  matrix \code{rfdist} derived from \code{dat}  and computes the AUCs for 
three logistic regression models: 
    (1) a model with the provided predictors in the formula, (2) a model with 
k = 1:K clusters generated from the \code{hclust} object, 
    and (3)  a model with k=1:K randomly generated clusters. 
    This is then repeated over \code{cv} cross-validation. See example below. 

Note that \code{dat} must be used to compute the dissimilarity matrix and passed to the function unchanged. 
  Otherwise there is no guarantee that the method will accurately assign clusters to the right observations 
in \code{dat}. Perhaps there might be a way to check for this?
}
\details{
\enumerate{
 \item \code{UnsupRFhclust} evaluates the predictive strength of the clusters using base \code{glm} 
 directly while \code{UnsupRFhclust.caret} uses glm through the caret package (caret package required). This offers 
an interphase to use different classifcation models in the caret package.  
 \item  The function \code{cluster_internal_validation} takes a hierarchical clustering model and a 
   dissimilarity matrix (e.g output of \code{\link{RFdist}}, and runs the cluster.stats function in the 
\code{fpc} package for K  different number of clusters and compute several 
internal cluster validation metrics and selects the best number of clusters by majority rule 
}
}
\examples{
\dontrun{
require(plyr)
require(ggplot2)
data(iris)
dat <- iris
 # get Random forest dissimilarity matrix 
RF.dist <- RFdist(data=dat[, -5], ntree = 10, no.rep=20, 
           syn.type = "permute", importance= FALSE)
form <- as.formula(paste0("Species ~ ", 
               paste0(setdiff(names(dat),c("Species")),collapse = "+")))

 # UnsupRFhclust 
res <- UnsupRFhclust(formula=form, dat=dat, rfdist = RF.dist$RFdist, K =20,  
                     parallel = FALSE, cv = 5, seed = 123)
tb <- ddply(res$perf, .variables = c("cluster", "type"), .fun = numcolwise(mean) )
 pp <- ggplot( ) +
 geom_line(data = tb, 
 aes(x = cluster , y = AUC, colour = type, linetype = type), size = 1.3) + 
 scale_color_manual(values= c("darkgreen", "darkred", "blue")) + 
 geom_vline(xintercept = 3, colour = "darkgreen") + 
 scale_x_continuous(name="Number of clusters",breaks=2:30) + ylab("AUC") +  
 ylim(c(0.55, 1)) +  
 theme(axis.title.x=element_text(size=14,face="bold"), 
 axis.title.y=element_text(size=14,face="bold"),
 legend.text = element_text(size=14,face="bold"), 
 axis.text.x = element_text(size = 13, face="bold",colour = "gray40"),
 legend.title = element_text(size=14,face="bold"),
 axis.text.y = element_text(size = 13, face="bold",colour = "gray40")) + 
 scale_linetype_manual(values=c("solid", "solid", "dotted"))
print(pp)

3 appears to be the best number of clusters 
clusters <- cutree(res$Hclust, 3)  

# cluster_internal_validation example  
# get hclust object from  UnsupRFhclust 
  HCmod = res$Hclust 
 rr <- cluster_internal_validation(K = 20, Hclust.model = HCmod,  
  RFdist = RF.dist$RFdist, seed = 1234)
 rr$table  
 rr$kopt 
}
}
\references{
Ngufor, C., Warner, M. A., Murphree, D. H., Liu, H., Carter, R., Storlie, C. B., & Kor, D. J. (2017). 
Identification of Clinically Meaningful Plasma Transfusion Subgroups Using Unsupervised Random Forest Clustering. 
In AMIA Annual Symposium Proceedings (Vol. 2017, p. 1332). American Medical Informatics Association.
}
